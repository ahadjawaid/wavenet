{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb094c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1c8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.LJSPEECH(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a904c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-7.3242e-04, -7.6294e-04, -6.4087e-04,  ...,  7.3242e-04,\n",
       "           2.1362e-04,  6.1035e-05]]),\n",
       " 22050,\n",
       " 'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform, sample_rate, transcript, normalized_transcript = dataset[0]\n",
    "\n",
    "waveform, sample_rate, normalized_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9de8e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 41885])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveforms = [dataset[i][0].view(1, 1, -1) for i in range(3)]\n",
    "\n",
    "smallest_size = min(list(map(lambda x: x.size(-1), waveforms)))\n",
    "clipped_inputs = list(map(lambda x: x[:,:,:smallest_size], waveforms))\n",
    "\n",
    "inputs = torch.stack(clipped_inputs).squeeze(2)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38d58d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as f\n",
    "\n",
    "# Represent speaker identity\n",
    "global_inputs = torch.stack((torch.tensor([[0]]), torch.tensor([[1]]), torch.tensor([[0]])))\n",
    "global_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1176913",
   "metadata": {},
   "source": [
    "Since the global input will be constant through out the entire speach generation we just have to expand the size of it to the number of time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fab93b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 41885)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_channels = global_inputs.size(1)\n",
    "batch_size, _, time_steps = inputs.size()\n",
    "batch_size, categories_channels, time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7f252a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 41885])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inputs + global_inputs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c780e0",
   "metadata": {},
   "source": [
    "it seems like we can just use brodcasting to add them together. Now lets upsample the local inputs to match time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fb1dd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 6])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_inputs = torch.stack([torch.randn((1,5,6)) for _ in range(3)]).squeeze(1)\n",
    "categories_channels = local_inputs.size(1)\n",
    "local_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a81519f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5d6efe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_inputs.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54310975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 41886])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "scaling_factor = ceil(time_steps / local_inputs.size(-1))\n",
    "upsampler = nn.ConvTranspose1d(categories_channels, categories_channels,scaling_factor,scaling_factor)\n",
    "upsampled_data = upsampler(local_inputs)\n",
    "\n",
    "upsampled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87c5a28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 41885])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inputs + upsampled_data[:,:,time_steps:]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32798ead",
   "metadata": {},
   "source": [
    "The consideration are now how to we get the scaling factor for each time step since in each dialted convolution the shape of the inputs get smaller so what must we do to get the correct size after each dialted convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0f017f",
   "metadata": {},
   "source": [
    "Since I don't know how to get the size of the upsampling convolution to work for different sizes I will just replicate the local size until it reach over the amount of time steps needed then clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39104c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 41885])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_inputs.repeat(1, 1, 6981)[:,:,:time_steps].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d5efee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
